
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>7. Rapid Iteration with Point Estimation Lotka-Volterra Dynamics &#8212; BayesFlow: Amortized Bayesian Inference</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "";
  </script>
  <!--
    this give us a css class that will be invisible only if js is disabled
  -->
  <noscript>
    <style>
      .pst-js-only { display: none !important; }

    </style>
  </noscript>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8878045cc6db502f8baf" rel="stylesheet" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=8f2a1f02" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-design.min.css?v=95c83b7e" />
    <link rel="stylesheet" type="text/css" href="../_static/custom.css?v=8fec244e" />
  
  <!-- So that users can add custom icons -->
  <script src="../_static/scripts/fontawesome.js?digest=8878045cc6db502f8baf"></script>
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf" />

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9bcbadda"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/design-tabs.js?v=f930bc37"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '_examples/Lotka_Volterra_point_estimation_and_expert_stats';</script>
    <script>
        DOCUMENTATION_OPTIONS.theme_version = '0.16.1';
        DOCUMENTATION_OPTIONS.theme_switcher_json_url = '/versions.json';
        DOCUMENTATION_OPTIONS.theme_switcher_version_match = 'dev';
        DOCUMENTATION_OPTIONS.show_version_warning_banner =
            false;
        </script>
    <link rel="canonical" href="https://www.bayesflow.org/_examples/Lotka_Volterra_point_estimation_and_expert_stats.html" />
    <link rel="icon" href="../_static/bayesflow_hex.ico"/>
    <link rel="author" title="About these documents" href="../about.html" />
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="API Reference" href="../api/bayesflow.html" />
    <link rel="prev" title="6. Simple Model Comparison - One Sample T-Test" href="One_Sample_TTest.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  <meta name="docsearch:version" content="" />
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <div id="pst-skip-link" class="skip-link d-print-none"><a href="#main-content">Skip to main content</a></div>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>Back to top</button>

  
  <dialog id="pst-search-dialog">
    
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         placeholder="Search the docs ..."
         aria-label="Search the docs ..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form>
  </dialog>

  <div class="pst-async-banner-revealer d-none">
  <aside id="bd-header-version-warning" class="d-none d-print-none" aria-label="Version warning"></aside>
</div>

  
    <header class="bd-header navbar navbar-expand-lg bd-navbar d-print-none">
<div class="bd-header__inner bd-page-width">
  <button class="pst-navbar-icon sidebar-toggle primary-toggle" aria-label="Site navigation">
    <span class="fa-solid fa-bars"></span>
  </button>
  
  
  <div class="col-lg-3 navbar-header-items__start">
    
      <div class="navbar-item">

  
    
  

<a class="navbar-brand logo" href="../index.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/bayesflow_hor.png" class="logo__image only-light" alt="BayesFlow: Amortized Bayesian Inference - Home"/>
    <img src="../_static/bayesflow_hor_dark.png" class="logo__image only-dark pst-js-only" alt="BayesFlow: Amortized Bayesian Inference - Home"/>
  
  
</a></div>
    
  </div>
  
  <div class="col-lg-9 navbar-header-items">
    
    <div class="me-auto navbar-header-items__center">
      
        <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-2"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-2"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-2"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-2">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
      
        <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../examples.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/bayesflow.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about.html">
    About us
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributing.html">
    Contributing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../development/index.html">
    Developer docs
  </a>
</li>

  </ul>
</nav></div>
      
    </div>
    
    
    <div class="navbar-header-items__end">
      
        <div class="navbar-item navbar-persistent--container">
          

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
        </div>
      
      
        <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
      
    </div>
    
  </div>
  
  
    <div class="navbar-persistent--mobile">

<button class="btn search-button-field search-button__button pst-js-only" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
 <i class="fa-solid fa-magnifying-glass"></i>
 <span class="search-button__default-text">Search</span>
 <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
</button>
    </div>
  

  
    <button class="pst-navbar-icon sidebar-toggle secondary-toggle" aria-label="On this page">
      <span class="fa-solid fa-outdent"></span>
    </button>
  
</div>

    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <dialog id="pst-primary-sidebar-modal"></dialog>
      <div id="pst-primary-sidebar" class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
      <div class="sidebar-header-items__center">
        
          
          
            <div class="navbar-item">
<div class="version-switcher__container dropdown pst-js-only">
  <button id="pst-version-switcher-button-3"
    type="button"
    class="version-switcher__button btn btn-sm dropdown-toggle"
    data-bs-toggle="dropdown"
    aria-haspopup="listbox"
    aria-controls="pst-version-switcher-list-3"
    aria-label="Version switcher list"
  >
    Choose version  <!-- this text may get changed later by javascript -->
    <span class="caret"></span>
  </button>
  <div id="pst-version-switcher-list-3"
    class="version-switcher__menu dropdown-menu list-group-flush py-0"
    role="listbox" aria-labelledby="pst-version-switcher-button-3">
    <!-- dropdown will be populated by javascript on page load -->
  </div>
</div></div>
          
        
          
          
            <div class="navbar-item">
<nav>
  <ul class="bd-navbar-elements navbar-nav">
    
<li class="nav-item current active">
  <a class="nav-link nav-internal" href="../examples.html">
    Examples
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../api/bayesflow.html">
    API Reference
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../about.html">
    About us
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../contributing.html">
    Contributing
  </a>
</li>


<li class="nav-item ">
  <a class="nav-link nav-internal" href="../development/index.html">
    Developer docs
  </a>
</li>

  </ul>
</nav></div>
          
        
      </div>
    
    
    
      <div class="sidebar-header-items__end">
        
          <div class="navbar-item">

<button class="btn btn-sm nav-link pst-navbar-icon theme-switch-button pst-js-only" aria-label="Color mode" data-bs-title="Color mode"  data-bs-placement="bottom" data-bs-toggle="tooltip">
  <i class="theme-switch fa-solid fa-sun                fa-lg" data-mode="light" title="Light"></i>
  <i class="theme-switch fa-solid fa-moon               fa-lg" data-mode="dark"  title="Dark"></i>
  <i class="theme-switch fa-solid fa-circle-half-stroke fa-lg" data-mode="auto"  title="System Settings"></i>
</button></div>
        
      </div>
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
<nav class="bd-docs-nav bd-links"
     aria-label="Section Navigation">
  <p class="bd-links__title" role="heading" aria-level="1">Section Navigation</p>
  <div class="bd-toc-item navbar-nav"><ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="Linear_Regression_Starter.html">1. Bayesian Linear Regression Starter</a></li>
<li class="toctree-l1"><a class="reference internal" href="Two_Moons_Starter.html">2. Two Moons: Tackling Bimodal Posteriors</a></li>
<li class="toctree-l1"><a class="reference internal" href="SIR_Posterior_Estimation.html">3. Posterior Estimation for SIR-like Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bayesian_Experimental_Design.html">4. Bayesian Experimental Design (BED) with BayesFlow and PyTorch</a></li>
<li class="toctree-l1"><a class="reference internal" href="From_ABC_to_BayesFlow.html">5. From ABC to BayesFlow</a></li>
<li class="toctree-l1"><a class="reference internal" href="One_Sample_TTest.html">6. Simple Model Comparison - One Sample T-Test</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">7. Rapid Iteration with Point Estimation Lotka-Volterra Dynamics</a></li>
</ul>
</div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
      <div class="sidebar-primary-item">
<div id="ethical-ad-placement"
      class="flat"
      data-ea-publisher="readthedocs"
      data-ea-type="readthedocs-sidebar"
      data-ea-manual="true">
</div></div>
  </div>


      </div>
      
      <main id="main-content" class="bd-main" role="main">
        
        
          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article d-print-none">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item">

<nav aria-label="Breadcrumb" class="d-print-none">
  <ul class="bd-breadcrumbs">
    
    <li class="breadcrumb-item breadcrumb-home">
      <a href="../index.html" class="nav-link" aria-label="Home">
        <i class="fa-solid fa-home"></i>
      </a>
    </li>
    
    <li class="breadcrumb-item"><a href="../examples.html" class="nav-link">Examples</a></li>
    
    <li class="breadcrumb-item active" aria-current="page"><span class="ellipsis"><span class="section-number">7. </span>Rapid Iteration with Point Estimation Lotka-Volterra Dynamics</span></li>
  </ul>
</nav>
</div>
      
    </div>
  
  
</div>
</div>
              
              
              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="rapid-iteration-with-point-estimation-lotka-volterra-dynamics">
<h1><span class="section-number">7. </span>Rapid Iteration with Point Estimation Lotka-Volterra Dynamics<a class="headerlink" href="#rapid-iteration-with-point-estimation-lotka-volterra-dynamics" title="Link to this heading">#</a></h1>
<p><em>Author: Hans Olischläger</em></p>
<p>In this notebook, we will infer parameters of a famous ecology differential equation with BayesFlow.</p>
<p>We will follow a typical workflow that emphazises rapid iterations early on, before building up towards reliable estimates of the full posterior with end-to-end data embedding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">matplotlib.pyplot</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">plt</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">numpy</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">np</span>
<span class="kn">import</span><span class="w"> </span><span class="nn">seaborn</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">sns</span>

<span class="kn">from</span><span class="w"> </span><span class="nn">scipy.integrate</span><span class="w"> </span><span class="kn">import</span> <span class="n">odeint</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">keras</span>

<span class="kn">import</span><span class="w"> </span><span class="nn">bayesflow</span><span class="w"> </span><span class="k">as</span><span class="w"> </span><span class="nn">bf</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>2025-03-15 18:01:29.115459: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-15 18:01:29.118881: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-15 18:01:29.126772: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
WARNING: All log messages before absl::InitializeLog() is called are written to STDERR
E0000 00:00:1742058089.139345  341836 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
E0000 00:00:1742058089.142967  341836 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-15 18:01:29.158501: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-15 18:01:30.958916: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># avoid scientific notation for outputs</span>
<span class="n">np</span><span class="o">.</span><span class="n">set_printoptions</span><span class="p">(</span><span class="n">suppress</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<section id="ecology-simulator">
<h2><span class="section-number">7.1. </span>Ecology simulator<a class="headerlink" href="#ecology-simulator" title="Link to this heading">#</a></h2>
<p>Say we measured population counts from two species over time. One of them preys on the other, so we might assume that the dynamics are governed by the classic Lotka-Volterra system.</p>
<p>In dimensionless form, with prey population <span class="math notranslate nohighlight">\(x\)</span> and predator population <span class="math notranslate nohighlight">\(y\)</span>, the nonlinear differential equation is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}{\frac {dx}{dt}}&amp;=\alpha x-\beta xy,\\{\frac {dy}{dt}}&amp;=-\gamma y+\delta xy.\end{aligned}
\end{split}\]</div>
<p>As always, this model entails a number of assumptions that can only be approximate. In brief:
On their own, prey count increases exponentially with rate <span class="math notranslate nohighlight">\(\alpha\)</span>, while predator count decays with rate <span class="math notranslate nohighlight">\(\gamma\)</span>.
Interesting dynamics are possible when both predators and prey are present: The number of predators increases the more prey it can hunt, reducing prey counts proportionally at a rate <span class="math notranslate nohighlight">\(\beta\)</span> and increasing predator count proportionally at a rate <span class="math notranslate nohighlight">\(\delta\)</span>.</p>
<p>We can measure population timeseries, but never the parameters directly, so this is a scientifically relevant inverse problem.</p>
<p>The Lotka-Volterra equations alone are not yet a concrete testable hypothesis, since it does not on its own predict anything measureable. We must pick parameters, initial conditions, and an observation model which describes how measurements take place. Note: the wide applicability of simulation-based inference is due to the fact that scientific hypotheses typically come in the form of simulators of measurable quantities.</p>
<p>Our simulator will consist of three parts:</p>
<ol class="arabic simple">
<li><p>First, we choose a prior distribution over parameters, that reflects our beliefs about parameters before observing data.</p></li>
<li><p>Building on parameters sampled from the prior, we solve the parameterized Lotka-Volterra equation in time starting from some initial conditions.</p></li>
<li><p>And finally, we hypothesize that we will make some counting errors when observing the populations, introducing a Gaussian error on the true populations.</p></li>
</ol>
<p>A random number generator with a fixed seed will ensure reproducibility of the simulated training and validation data.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">rng</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">default_rng</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="mi">1234</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">prior</span><span class="p">():</span>
    <span class="n">x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="mi">4</span><span class="p">)</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="mi">1</span><span class="o">/</span><span class="p">(</span><span class="mi">1</span><span class="o">+</span><span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="o">-</span><span class="n">x</span><span class="p">))</span> <span class="o">*</span> <span class="mf">3.9</span> <span class="o">+</span> <span class="mf">0.1</span> <span class="c1"># logit normal distribution scaled to range from 0.1 and 4</span>
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">alpha</span><span class="o">=</span><span class="n">theta</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span>
        <span class="n">beta</span><span class="o">=</span><span class="n">theta</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span>
        <span class="n">gamma</span><span class="o">=</span><span class="n">theta</span><span class="p">[</span><span class="mi">2</span><span class="p">],</span>
        <span class="n">delta</span><span class="o">=</span><span class="n">theta</span><span class="p">[</span><span class="mi">3</span><span class="p">],</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">lotka_volterra_equations</span><span class="p">(</span><span class="n">state</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">delta</span><span class="p">):</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">state</span>
    <span class="n">dxdt</span> <span class="o">=</span> <span class="n">alpha</span> <span class="o">*</span> <span class="n">x</span> <span class="o">-</span> <span class="n">beta</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
    <span class="n">dydt</span> <span class="o">=</span> <span class="o">-</span> <span class="n">gamma</span> <span class="o">*</span> <span class="n">y</span> <span class="o">+</span> <span class="n">delta</span> <span class="o">*</span> <span class="n">x</span> <span class="o">*</span> <span class="n">y</span>
    <span class="k">return</span> <span class="p">[</span><span class="n">dxdt</span><span class="p">,</span> <span class="n">dydt</span><span class="p">]</span>

<span class="k">def</span><span class="w"> </span><span class="nf">ecology_model</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">delta</span><span class="p">,</span> <span class="n">t_span</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">t_steps</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">initial_state</span><span class="o">=</span><span class="p">[</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">]):</span>
    <span class="n">t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="n">t_span</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">t_span</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">t_steps</span><span class="p">)</span>
    <span class="n">state</span> <span class="o">=</span> <span class="n">odeint</span><span class="p">(</span><span class="n">lotka_volterra_equations</span><span class="p">,</span> <span class="n">initial_state</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">args</span><span class="o">=</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">,</span> <span class="n">gamma</span><span class="p">,</span> <span class="n">delta</span><span class="p">))</span>
    <span class="n">x</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">T</span>  <span class="c1"># Transpose to get x and y arrays</span>
    
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">x</span><span class="o">=</span><span class="n">x</span><span class="p">,</span>  <span class="c1"># Prey time series</span>
        <span class="n">y</span><span class="o">=</span><span class="n">y</span><span class="p">,</span>  <span class="c1"># Predator time series</span>
        <span class="n">t</span><span class="o">=</span><span class="n">t</span><span class="p">,</span>  <span class="c1"># time</span>
    <span class="p">)</span>

<span class="k">def</span><span class="w"> </span><span class="nf">observation_model</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">t</span><span class="p">,</span> <span class="n">observation_subsampling</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">observation_probability</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">observation_noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">):</span>
    <span class="n">t_steps</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="c1"># observation noise</span>
    <span class="n">observed_x</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">x</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">observation_noise</span><span class="p">)</span>
    <span class="n">observed_y</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="n">y</span><span class="p">,</span> <span class="n">scale</span><span class="o">=</span><span class="n">observation_noise</span><span class="p">)</span>
    <span class="n">observed_t</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">copy</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>

    <span class="c1"># if observation_probability &lt; 1, the population count is missing for some time steps</span>
    <span class="n">random_indices</span> <span class="o">=</span> <span class="n">rng</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">t_steps</span><span class="p">,</span> <span class="n">observation_subsampling</span><span class="p">),</span> <span class="nb">int</span><span class="p">(</span><span class="n">observation_probability</span> <span class="o">*</span> <span class="n">t_steps</span> <span class="o">//</span> <span class="n">observation_subsampling</span><span class="p">),</span> <span class="n">replace</span><span class="o">=</span><span class="kc">False</span><span class="p">)</span>
    <span class="n">random_indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sort</span><span class="p">(</span><span class="n">random_indices</span><span class="p">)</span>  <span class="c1"># rng.choice scrambles the order of observation indices</span>
    
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">observed_x</span><span class="o">=</span><span class="n">observed_x</span><span class="p">[</span><span class="n">random_indices</span><span class="p">],</span>  <span class="c1"># Prey time series</span>
        <span class="n">observed_y</span><span class="o">=</span><span class="n">observed_y</span><span class="p">[</span><span class="n">random_indices</span><span class="p">],</span>  <span class="c1"># Predator time series</span>
        <span class="n">observed_t</span><span class="o">=</span><span class="n">observed_t</span><span class="p">[</span><span class="n">random_indices</span><span class="p">],</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can combine these three components into a BayesFlow simulator via:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simulator</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">make_simulator</span><span class="p">([</span><span class="n">prior</span><span class="p">,</span> <span class="n">ecology_model</span><span class="p">,</span> <span class="n">observation_model</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<p>Let’s sample 1000 trajectories, and see what we get:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_trajectories</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">samples</span> <span class="o">=</span> <span class="n">simulator</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">num_trajectories</span><span class="p">)</span>
<span class="n">keras</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">samples</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: (1000, 1),
 &#39;beta&#39;: (1000, 1),
 &#39;gamma&#39;: (1000, 1),
 &#39;delta&#39;: (1000, 1),
 &#39;x&#39;: (1000, 100),
 &#39;y&#39;: (1000, 100),
 &#39;t&#39;: (1000, 100),
 &#39;observed_x&#39;: (1000, 10),
 &#39;observed_y&#39;: (1000, 10),
 &#39;observed_t&#39;: (1000, 10)}
</pre></div>
</div>
</div>
</div>
<p>What types of developments (and observations) does our Lotka-Volterra simulator predict? We should have a function to visualize sampled trajectories and take a look!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">trajectory_aggregation</span><span class="p">(</span><span class="n">traj</span><span class="p">,</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">):</span>
    <span class="n">alpha</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">confidence</span>
    <span class="n">quantiles</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">quantile</span><span class="p">(</span><span class="n">traj</span><span class="p">,</span> <span class="p">[</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="o">/</span><span class="mi">2</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span><span class="o">.</span><span class="n">T</span>
    <span class="n">central</span> <span class="o">=</span> <span class="n">quantiles</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">L</span> <span class="o">=</span> <span class="n">quantiles</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span>
    <span class="n">U</span> <span class="o">=</span> <span class="n">quantiles</span><span class="p">[:,</span><span class="mi">2</span><span class="p">]</span>
    <span class="k">return</span> <span class="n">central</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">U</span>

<span class="k">def</span><span class="w"> </span><span class="nf">plot_trajectores</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="n">variable_keys</span><span class="p">,</span> <span class="n">variable_names</span><span class="p">,</span> <span class="n">fill_colors</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;blue&quot;</span><span class="p">,</span> <span class="s2">&quot;darkred&quot;</span><span class="p">],</span> <span class="n">confidence</span><span class="o">=</span><span class="mf">0.95</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.8</span><span class="p">,</span> <span class="n">observations</span><span class="o">=</span><span class="kc">None</span><span class="p">,</span> <span class="n">ax</span><span class="o">=</span><span class="kc">None</span><span class="p">):</span>
    <span class="n">t_span</span> <span class="o">=</span> <span class="n">samples</span><span class="p">[</span><span class="s2">&quot;t&quot;</span><span class="p">][</span><span class="mi">0</span><span class="p">]</span>
    
    <span class="k">if</span> <span class="n">ax</span> <span class="ow">is</span> <span class="kc">None</span><span class="p">:</span>
        <span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span><span class="mi">3</span><span class="p">))</span>
        <span class="n">sns</span><span class="o">.</span><span class="n">despine</span><span class="p">()</span>
    
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">key</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">variable_keys</span><span class="p">):</span>

        <span class="k">if</span> <span class="n">observations</span> <span class="ow">is</span> <span class="ow">not</span> <span class="kc">None</span><span class="p">:</span>     
            <span class="n">ax</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">observations</span><span class="p">[</span><span class="s2">&quot;observed_t&quot;</span><span class="p">],</span> <span class="n">observations</span><span class="p">[</span><span class="s2">&quot;observed_&quot;</span><span class="o">+</span><span class="n">key</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">fill_colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">marker</span><span class="o">=</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Observed &quot;</span> <span class="o">+</span> <span class="n">variable_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>

        <span class="n">central</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">U</span> <span class="o">=</span> <span class="n">trajectory_aggregation</span><span class="p">(</span><span class="n">samples</span><span class="p">[</span><span class="n">key</span><span class="p">],</span> <span class="n">confidence</span><span class="o">=</span><span class="n">confidence</span><span class="p">)</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_span</span><span class="p">,</span> <span class="n">central</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">fill_colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;Median &quot;</span> <span class="o">+</span> <span class="n">variable_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">lower</span><span class="p">())</span>
        <span class="n">ax</span><span class="o">.</span><span class="n">fill_between</span><span class="p">(</span><span class="n">t_span</span><span class="p">,</span> <span class="n">L</span><span class="p">,</span> <span class="n">U</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="n">fill_colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">rf</span><span class="s2">&quot;</span><span class="si">{</span><span class="nb">int</span><span class="p">((</span><span class="n">confidence</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="mi">100</span><span class="p">)</span><span class="si">}</span><span class="s2">$\%$ Confidence Bands&quot;</span><span class="p">)</span>

        <span class="c1"># plot 20 trajectory samples</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">20</span><span class="p">):</span>
            <span class="k">if</span> <span class="n">j</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
                <span class="n">label</span> <span class="o">=</span> <span class="sa">f</span><span class="s2">&quot;</span><span class="si">{</span><span class="n">variable_names</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s2"> trajectories&quot;</span>
            <span class="k">else</span><span class="p">:</span>
                <span class="n">label</span> <span class="o">=</span> <span class="kc">None</span>
            <span class="n">ax</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">t_span</span><span class="p">,</span> <span class="n">samples</span><span class="p">[</span><span class="n">key</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">fill_colors</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="n">label</span><span class="p">)</span>
        

    <span class="n">ax</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s2">&quot;t&quot;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s2">&quot;population&quot;</span><span class="p">)</span>

<span class="n">plot_trajectores</span><span class="p">(</span><span class="n">samples</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Prey&quot;</span><span class="p">,</span> <span class="s2">&quot;Predator&quot;</span><span class="p">])</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/b6676f7060f964857feb5600ca259dcbf704feb5921d7681be0f4a3107594f9d.png" src="../_images/b6676f7060f964857feb5600ca259dcbf704feb5921d7681be0f4a3107594f9d.png" />
</div>
</div>
<p>Above we see the prior predictive distribution of the simulator. The shaded area contains 95% of trajectories at each timestep, additionally we see a few example trajectories.</p>
<p>Predator and prey populations generally oscillate in this model. But the frequency, amplitude, relative lag and scale varies greatly for different parameters.</p>
<p>The prior predictive distribution should match our expectation of the real world system of interest before we take into account concrete observed population counts.
Here, we see the prior implies population magnitudes to oscillate (mostly) below 6.</p>
</section>
<section id="rapid-inference">
<h2><span class="section-number">7.2. </span>Rapid inference<a class="headerlink" href="#rapid-inference" title="Link to this heading">#</a></h2>
<p>The first goal will be to get a fast but crude approximation of the true posteriors for different observations. Two ingredients will allow us to move fast towards parameter inference:</p>
<ol class="arabic simple">
<li><p>basic hand crafted summary statistics</p></li>
<li><p>point estimation</p></li>
</ol>
<p>This will help us diagnose challenges with the simulator and establishes a baseline for the final goal: full posterior inference.</p>
<section id="basic-hand-crafted-summary-statistics">
<h3><span class="section-number">7.2.1. </span>Basic hand crafted summary statistics<a class="headerlink" href="#basic-hand-crafted-summary-statistics" title="Link to this heading">#</a></h3>
<p>Ultimately, we want to learn maximally informative summary statistics jointly with an amortized posterior approximation, but hand crafted summary statistics have the benefit of being interpretable and fast to compute. Oftentimes, there are a few natural and established statistics for a particular modality of raw data. Researchers of the field are likely to have made significant progress in finding closed form expressions or algorithms for informative summaries.</p>
<p>Compared to the theoretically optimal summary statistics, we can expect there to be less posterior contraction.</p>
<p>Still, we can reasonably expect, that oscillation period, mean, (log) variance, autocorrelation at different lags of both trajectories, and the cross correlation between the two trajectories are highly informative when taken together as summary statistics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span><span class="w"> </span><span class="nn">scipy</span>

<span class="k">def</span><span class="w"> </span><span class="nf">period</span><span class="p">(</span><span class="n">observed_x</span><span class="p">,</span> <span class="n">t_span</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mi">5</span><span class="p">],</span> <span class="n">t_steps</span><span class="o">=</span><span class="mi">500</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the dominant period of observed_x from a periodogram.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">f</span><span class="p">,</span> <span class="n">Pxx</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">signal</span><span class="o">.</span><span class="n">periodogram</span><span class="p">(</span><span class="n">observed_x</span><span class="p">,</span> <span class="n">t_steps</span><span class="o">/</span><span class="p">(</span><span class="n">t_span</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">-</span><span class="n">t_span</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
    <span class="n">freq_dominant</span> <span class="o">=</span> <span class="n">f</span><span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">Pxx</span><span class="p">)]</span>
    <span class="n">T</span> <span class="o">=</span> <span class="mi">1</span> <span class="o">/</span> <span class="n">freq_dominant</span>
    <span class="k">return</span> <span class="n">T</span>


<span class="k">def</span><span class="w"> </span><span class="nf">autocorr</span><span class="p">(</span><span class="n">trajectory</span><span class="p">,</span> <span class="n">lags</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the autocorrelation for each specified lag in a trajectory.</span>
<span class="sd">    </span>
<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    trajectory : np.ndarray</span>
<span class="sd">        The time series data, assumed to be a 1D array.</span>
<span class="sd">    lags : np.ndarray or list</span>
<span class="sd">        The lags at which to compute the autocorrelation.</span>
<span class="sd">    </span>
<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    auto_correlation : np.ndarray</span>
<span class="sd">        Autocorrelation values at each specified lag.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Calculate the mean and variance of the trajectory for normalization</span>
    <span class="n">mean</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>
    <span class="n">var</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">var</span><span class="p">(</span><span class="n">trajectory</span><span class="p">)</span>
    
    <span class="c1"># Initialize an array to hold the autocorrelation values</span>
    <span class="n">auto_correlation</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">lags</span><span class="p">))</span>
    
    <span class="c1"># Compute autocorrelation for each lag</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">lag</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">lags</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">lag</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
            <span class="c1"># Autocorrelation at lag 0 is always 1</span>
            <span class="n">auto_correlation</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">1</span>
        <span class="k">elif</span> <span class="n">lag</span> <span class="o">&gt;=</span> <span class="nb">len</span><span class="p">(</span><span class="n">trajectory</span><span class="p">):</span>
            <span class="c1"># If the lag is equal to or greater than the length of the trajectory, autocorrelation is undefined (set to 0)</span>
            <span class="n">auto_correlation</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Compute covariance and then autocorrelation</span>
            <span class="n">cov</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">trajectory</span><span class="p">[:</span><span class="o">-</span><span class="n">lag</span><span class="p">]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">trajectory</span><span class="p">[</span><span class="n">lag</span><span class="p">:]</span> <span class="o">-</span> <span class="n">mean</span><span class="p">))</span>
            <span class="n">auto_correlation</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">cov</span> <span class="o">/</span> <span class="n">var</span>

    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">any</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">isnan</span><span class="p">(</span><span class="n">auto_correlation</span><span class="p">)):</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">auto_correlation</span><span class="p">)</span>
            
    <span class="k">return</span> <span class="n">auto_correlation</span>

<span class="k">def</span><span class="w"> </span><span class="nf">crosscorr</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;</span>
<span class="sd">    Computes the cross-correlation (Pearson correlation coefficient) between two trajectories at zero lag.</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    x : np.ndarray</span>
<span class="sd">        The first time series data, assumed to be a 1D array of length n.</span>
<span class="sd">    y : np.ndarray</span>
<span class="sd">        The second time series data, assumed to be a 1D array of length n.</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    float</span>
<span class="sd">        The cross-correlation coefficient.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="c1"># Compute the mean and standard deviation of both time series</span>
    <span class="n">mean_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">mean_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
    <span class="n">std_x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
    <span class="n">std_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">std</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>

    <span class="c1"># Compute the covariance and the correlation coefficient</span>
    <span class="n">covariance</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">((</span><span class="n">x</span> <span class="o">-</span> <span class="n">mean_x</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">y</span> <span class="o">-</span> <span class="n">mean_y</span><span class="p">))</span>
    <span class="n">correlation</span> <span class="o">=</span> <span class="n">covariance</span> <span class="o">/</span> <span class="p">(</span><span class="n">std_x</span> <span class="o">*</span> <span class="n">std_y</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">correlation</span>

<span class="k">def</span><span class="w"> </span><span class="nf">expert_stats</span><span class="p">(</span><span class="n">observed_x</span><span class="p">,</span> <span class="n">observed_y</span><span class="p">,</span> <span class="n">lags</span><span class="o">=</span><span class="p">[</span><span class="mi">2</span><span class="p">,</span><span class="mi">5</span><span class="p">]):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;Computes fixed size statistics for an observed population trajectory</span>

<span class="sd">    Parameters</span>
<span class="sd">    ----------</span>
<span class="sd">    observed_x : np.ndarray with shape (num_observations, )</span>
<span class="sd">    observed_y : np.ndarray with shape (num_observations, )</span>

<span class="sd">    Returns</span>
<span class="sd">    -------</span>
<span class="sd">    dictionary with the following keys and values</span>
<span class="sd">    means      : np.ndarray with shape (2,)</span>
<span class="sd">    log_vars   : np.ndarray with shape (2,)</span>
<span class="sd">    auto_corrs : np.ndarray with shape (2*num_lags,)</span>
<span class="sd">        auto-correlation of each timeseries at lags 0.2 and 0.4 time units</span>
<span class="sd">    cross_corr : np.ndarray with shape (1,)</span>
<span class="sd">        the cross-correlation between the two time series</span>
<span class="sd">    period     : np.ndarray with shape (1,)</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="n">means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">observed_x</span><span class="o">.</span><span class="n">mean</span><span class="p">(),</span> <span class="n">observed_y</span><span class="o">.</span><span class="n">mean</span><span class="p">()])</span>
    <span class="n">log_vars</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="n">observed_x</span><span class="o">.</span><span class="n">var</span><span class="p">(),</span> <span class="n">observed_y</span><span class="o">.</span><span class="n">var</span><span class="p">()]))</span>
    <span class="n">auto_corrs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span>
        <span class="n">autocorr</span><span class="p">(</span><span class="n">observed_x</span><span class="p">,</span><span class="n">lags</span><span class="p">),</span>
        <span class="n">autocorr</span><span class="p">(</span><span class="n">observed_y</span><span class="p">,</span><span class="n">lags</span><span class="p">),</span>
    <span class="p">])</span><span class="o">.</span><span class="n">flatten</span><span class="p">()</span>
    <span class="n">cross_corr</span> <span class="o">=</span> <span class="n">crosscorr</span><span class="p">(</span><span class="n">observed_x</span><span class="p">,</span> <span class="n">observed_y</span><span class="p">)</span>
    <span class="n">T</span> <span class="o">=</span> <span class="n">period</span><span class="p">(</span><span class="n">observed_x</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="nb">dict</span><span class="p">(</span>
        <span class="n">means</span><span class="o">=</span><span class="n">means</span><span class="p">,</span>
        <span class="n">log_vars</span><span class="o">=</span><span class="n">log_vars</span><span class="p">,</span>
        <span class="n">auto_corrs</span><span class="o">=</span><span class="n">auto_corrs</span><span class="p">,</span>
        <span class="n">cross_corr</span><span class="o">=</span><span class="n">cross_corr</span><span class="p">,</span>
        <span class="n">period</span><span class="o">=</span><span class="n">T</span><span class="p">,</span>
    <span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>To compute the expert statistics we can append the <code class="docutils literal notranslate"><span class="pre">expert_stats</span></code> function to the <code class="docutils literal notranslate"><span class="pre">simulator</span></code> object.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">simulator</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">make_simulator</span><span class="p">([</span><span class="n">prior</span><span class="p">,</span> <span class="n">ecology_model</span><span class="p">,</span> <span class="n">observation_model</span><span class="p">,</span> <span class="n">expert_stats</span><span class="p">])</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">samples_with_expert_stats</span> <span class="o">=</span> <span class="n">simulator</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">3</span><span class="p">)</span>
<span class="n">keras</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">samples_with_expert_stats</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: (3, 1),
 &#39;beta&#39;: (3, 1),
 &#39;gamma&#39;: (3, 1),
 &#39;delta&#39;: (3, 1),
 &#39;x&#39;: (3, 100),
 &#39;y&#39;: (3, 100),
 &#39;t&#39;: (3, 100),
 &#39;observed_x&#39;: (3, 10),
 &#39;observed_y&#39;: (3, 10),
 &#39;observed_t&#39;: (3, 10),
 &#39;means&#39;: (3, 2),
 &#39;log_vars&#39;: (3, 2),
 &#39;auto_corrs&#39;: (3, 4),
 &#39;cross_corr&#39;: (3, 1),
 &#39;period&#39;: (3, 1)}
</pre></div>
</div>
</div>
</div>
</section>
<section id="point-estimation">
<h3><span class="section-number">7.2.2. </span>Point estimation<a class="headerlink" href="#point-estimation" title="Link to this heading">#</a></h3>
<p>Ultimately, we want to infer the full posterior distribution, but it can be much faster to infer point estimates of the same and already allow us to diagnose whether inference is or can be successful for a particular simulator.
Thus, in the spirit of rapid iteration we will first target the posterior mean and a few quantiles.</p>
<p>BayesFlow provides a convenient interface for point estimation. Here is a brief explantion of the principle:</p>
<p>Each point estimator is obtained by minimizing the Bayes risk for a particular loss function. Depending on the loss function, the resulting estimator will faithfully estimate a different functional of the full posterior distribution.</p>
<p>Typically, we refer to such loss functions as <em>scores</em> or <em>scoring rules</em> for a particular probabilistic forecast, since they score forecasts of a distribution <span class="math notranslate nohighlight">\(p(\theta|x)\)</span> based on samples <span class="math notranslate nohighlight">\(\theta \sim p(\theta|x)\)</span> of that distribution.
If the true forecast is the best forecast under the score, i.e. optimizes the score (uniquely), such losses are called <strong>(strictly) proper scoring rules</strong>.</p>
<ul>
<li><p>Here is a strictly proper scoring rule that is optimal if the estimate, <span class="math notranslate nohighlight">\(\hat \theta\)</span>, is the true <strong>mean</strong> of the posterior:</p>
<div class="math notranslate nohighlight">
\[L(\hat \theta, \theta; k) = | \theta - \hat \theta |^2\]</div>
<p>It is the well known squared error loss!</p>
</li>
<li><p>Similarly, since median minimizes the expected absolute distance to <span class="math notranslate nohighlight">\(\theta \sim p(\theta|x)\)</span>, we know that the corresponding loss is optimized by the true <strong>median</strong> of the posterior.</p>
<div class="math notranslate nohighlight">
\[L(\hat \theta, \theta; k) = | \theta - \hat \theta |\]</div>
</li>
<li><p>To estimate <strong>quantiles</strong>, the following is a strictly proper scoring rule:</p>
<div class="math notranslate nohighlight">
\[L(\hat \theta, \theta; \tau) = (\hat \theta - \theta)(\mathbf{1}_{\hat \theta - \theta &gt; 0} - \tau)\]</div>
<p>Here we write an indicator function as <span class="math notranslate nohighlight">\(\mathbf{1}_{\hat \theta - \theta &gt; 0}\)</span> to evaluate to 1 for overestimation (positive <span class="math notranslate nohighlight">\(\hat \theta - \theta\)</span>) and <span class="math notranslate nohighlight">\(0\)</span> otherwise.</p>
<p>For <span class="math notranslate nohighlight">\(\tau=\frac 1 2\)</span>, over- or underestimating a true posterior sample <span class="math notranslate nohighlight">\(\theta\)</span> is weighted equally. In fact, the quantile loss with <span class="math notranslate nohighlight">\(\tau=\frac 1 2\)</span> is identical to the median loss (up to a scaling of <span class="math notranslate nohighlight">\(\frac 1 2\)</span>). For the same reasons, both estimate the median of the posterior.</p>
<p>More generally, <span class="math notranslate nohighlight">\(\tau \in (0,1)\)</span> is the quantile level, that is the point where to evaluate the <a class="reference external" href="https://en.wikipedia.org/wiki/Quantile_function">quantile function</a>.</p>
</li>
<li><p>Note, that when approximating the full distribution in BayesFlow we score a <strong>probability estimate</strong> <span class="math notranslate nohighlight">\(\hat p(\theta|x)\)</span> with the log-score,</p>
<div class="math notranslate nohighlight">
\[L(\hat p(\theta|x), \theta) = \log (\hat p(\theta))\]</div>
<p>which is also a strictly proper scoring rule.</p>
</li>
<li><p>What if you want to estimate something else? There might just be a loss function that corresponds to the estimator of exactly the quantity you are after.</p>
<p>The class of functions that leads to faithful estimators is called <em>strictly proper scoring rules</em>.
A good reference for the theory and examples is the following <a class="reference external" href="https://doi.org/10.1198/016214506000001437">paper</a>.</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>  Gneiting, T., &amp; Raftery, A. E. (2007). Strictly Proper Scoring Rules, Prediction, and Estimation. Journal of the American Statistical Association, 102(477), 359–378. https://doi.org/10.1198/016214506000001437
</pre></div>
</div>
</li>
</ul>
<p>If you can find a proper scoring rule for the quantity you want to estimate, implement it as a negatively-oriented loss function, inherit from the abstract <code class="docutils literal notranslate"><span class="pre">ScoringRule</span></code> class and you will be able to use it within BayesFlow.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">adapter</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">bf</span><span class="o">.</span><span class="n">adapters</span><span class="o">.</span><span class="n">Adapter</span><span class="p">()</span>
    
    <span class="c1"># convert any non-arrays to numpy arrays</span>
    <span class="o">.</span><span class="n">to_array</span><span class="p">()</span>
    
    <span class="c1"># convert from numpy&#39;s default float64 to deep learning friendly float32</span>
    <span class="o">.</span><span class="n">convert_dtype</span><span class="p">(</span><span class="s2">&quot;float64&quot;</span><span class="p">,</span> <span class="s2">&quot;float32&quot;</span><span class="p">)</span>

    <span class="c1"># drop unobserved full trajectories and raw observations</span>
    <span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;t&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_x&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_y&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_t&quot;</span><span class="p">])</span>
    
    <span class="c1"># standardize hand-crafted statistics to zero mean and unit variance </span>
    <span class="o">.</span><span class="n">standardize</span><span class="p">()</span><span class="c1">#include=[&quot;means&quot;, &quot;log_vars&quot;, &quot;auto_corrs&quot;, &quot;cross_corr&quot;, &quot;period&quot;])</span>
    
    <span class="c1"># rename the variables to match the required approximator inputs</span>
    <span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="s2">&quot;delta&quot;</span><span class="p">],</span> <span class="n">into</span><span class="o">=</span><span class="s2">&quot;inference_variables&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="s2">&quot;means&quot;</span><span class="p">,</span> <span class="s2">&quot;log_vars&quot;</span><span class="p">,</span> <span class="s2">&quot;auto_corrs&quot;</span><span class="p">,</span> <span class="s2">&quot;cross_corr&quot;</span><span class="p">,</span> <span class="s2">&quot;period&quot;</span><span class="p">],</span> <span class="n">into</span><span class="o">=</span><span class="s2">&quot;inference_conditions&quot;</span><span class="p">)</span>

<span class="p">)</span>
<span class="n">adapter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adapter([0: ToArray -&gt; 1: ConvertDType -&gt; 2: Drop([&#39;x&#39;, &#39;y&#39;, &#39;t&#39;, &#39;observed_x&#39;, &#39;observed_y&#39;, &#39;observed_t&#39;]) -&gt; 3: Standardize -&gt; 4: Concatenate([&#39;alpha&#39;, &#39;beta&#39;, &#39;gamma&#39;, &#39;delta&#39;] -&gt; &#39;inference_variables&#39;) -&gt; 5: Concatenate([&#39;means&#39;, &#39;log_vars&#39;, &#39;auto_corrs&#39;, &#39;cross_corr&#39;, &#39;period&#39;] -&gt; &#39;inference_conditions&#39;)])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_training_batches</span> <span class="o">=</span> <span class="mi">512</span>
<span class="n">num_validation_batches</span> <span class="o">=</span> <span class="mi">128</span>
<span class="n">batch_size</span> <span class="o">=</span> <span class="mi">64</span>
<span class="n">epochs</span> <span class="o">=</span> <span class="mi">10</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_training_batches</span> <span class="o">*</span> <span class="n">batch_size</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>32768
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">training_data</span> <span class="o">=</span> <span class="n">simulator</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">num_training_batches</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,)</span>
<span class="n">validation_data</span> <span class="o">=</span> <span class="n">simulator</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">num_validation_batches</span> <span class="o">*</span> <span class="n">batch_size</span><span class="p">,)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 32.1 s, sys: 336 ms, total: 32.4 s
Wall time: 32.3 s
</pre></div>
</div>
</div>
</div>
<p><code class="docutils literal notranslate"><span class="pre">PointInferenceNetwork</span></code>s are defined by the <code class="docutils literal notranslate"><span class="pre">ScoringRule</span></code>s they use to approximate certain point estimates. Passing a dictionary of such <code class="docutils literal notranslate"><span class="pre">ScoringRule</span></code>s will construct a corresponding feed forward model.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">q_levels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mf">0.1</span><span class="p">,</span><span class="mf">0.9</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>

<span class="n">point_inference_network</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">PointInferenceNetwork</span><span class="p">(</span>
    <span class="n">scores</span><span class="o">=</span><span class="nb">dict</span><span class="p">(</span>
        <span class="n">mean</span><span class="o">=</span><span class="n">bf</span><span class="o">.</span><span class="n">scores</span><span class="o">.</span><span class="n">MeanScore</span><span class="p">(),</span>
        <span class="n">quantiles</span><span class="o">=</span><span class="n">bf</span><span class="o">.</span><span class="n">scores</span><span class="o">.</span><span class="n">QuantileScore</span><span class="p">(</span><span class="n">q_levels</span><span class="p">),</span>
    <span class="p">),</span>
<span class="p">)</span>

<span class="n">point_inference_workflow</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">BasicWorkflow</span><span class="p">(</span>
    <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
    <span class="n">adapter</span><span class="o">=</span><span class="n">adapter</span><span class="p">,</span>
    <span class="n">inference_network</span><span class="o">=</span><span class="n">point_inference_network</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">point_inference_workflow</span><span class="o">.</span><span class="n">fit_offline</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">,</span>
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_data</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:bayesflow:Fitting on dataset instance of OfflineDataset.
INFO:bayesflow:Building on a test batch.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">3s</span> 3ms/step - loss: 0.1912 - loss/inference_loss: 0.1912 - mean/inference_mean: 0.2326 - quantiles/inference_quantiles: 0.1497 - val_loss: 0.1242 - val_loss/inference_loss: 0.1242 - val_mean/inference_mean: 0.1398 - val_quantiles/inference_quantiles: 0.1087
Epoch 2/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 2ms/step - loss: 0.1387 - loss/inference_loss: 0.1387 - mean/inference_mean: 0.1659 - quantiles/inference_quantiles: 0.1115 - val_loss: 0.1337 - val_loss/inference_loss: 0.1337 - val_mean/inference_mean: 0.1546 - val_quantiles/inference_quantiles: 0.1127
Epoch 3/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 2ms/step - loss: 0.1313 - loss/inference_loss: 0.1313 - mean/inference_mean: 0.1562 - quantiles/inference_quantiles: 0.1065 - val_loss: 0.1371 - val_loss/inference_loss: 0.1371 - val_mean/inference_mean: 0.1680 - val_quantiles/inference_quantiles: 0.1062
Epoch 4/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 3ms/step - loss: 0.1267 - loss/inference_loss: 0.1267 - mean/inference_mean: 0.1501 - quantiles/inference_quantiles: 0.1034 - val_loss: 0.1605 - val_loss/inference_loss: 0.1605 - val_mean/inference_mean: 0.2026 - val_quantiles/inference_quantiles: 0.1184
Epoch 5/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 2ms/step - loss: 0.1238 - loss/inference_loss: 0.1238 - mean/inference_mean: 0.1459 - quantiles/inference_quantiles: 0.1016 - val_loss: 0.1139 - val_loss/inference_loss: 0.1139 - val_mean/inference_mean: 0.1323 - val_quantiles/inference_quantiles: 0.0955
Epoch 6/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 2ms/step - loss: 0.1212 - loss/inference_loss: 0.1212 - mean/inference_mean: 0.1423 - quantiles/inference_quantiles: 0.1002 - val_loss: 0.1012 - val_loss/inference_loss: 0.1012 - val_mean/inference_mean: 0.1103 - val_quantiles/inference_quantiles: 0.0922
Epoch 7/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 2ms/step - loss: 0.1193 - loss/inference_loss: 0.1193 - mean/inference_mean: 0.1396 - quantiles/inference_quantiles: 0.0990 - val_loss: 0.1072 - val_loss/inference_loss: 0.1072 - val_mean/inference_mean: 0.1234 - val_quantiles/inference_quantiles: 0.0909
Epoch 8/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 2ms/step - loss: 0.1177 - loss/inference_loss: 0.1177 - mean/inference_mean: 0.1373 - quantiles/inference_quantiles: 0.0981 - val_loss: 0.1230 - val_loss/inference_loss: 0.1230 - val_mean/inference_mean: 0.1430 - val_quantiles/inference_quantiles: 0.1029
Epoch 9/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 2ms/step - loss: 0.1167 - loss/inference_loss: 0.1167 - mean/inference_mean: 0.1358 - quantiles/inference_quantiles: 0.0976 - val_loss: 0.1103 - val_loss/inference_loss: 0.1103 - val_mean/inference_mean: 0.1251 - val_quantiles/inference_quantiles: 0.0954
Epoch 10/10
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">1s</span> 2ms/step - loss: 0.1161 - loss/inference_loss: 0.1161 - mean/inference_mean: 0.1349 - quantiles/inference_quantiles: 0.0973 - val_loss: 0.1229 - val_loss/inference_loss: 0.1229 - val_mean/inference_mean: 0.1458 - val_quantiles/inference_quantiles: 0.1000
CPU times: user 29.8 s, sys: 2.66 s, total: 32.4 s
Wall time: 13.8 s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/897f3070a52ac93ae9dc0462b232e8b3474825eb83b5e0ec727e31e31cec3a40.png" src="../_images/897f3070a52ac93ae9dc0462b232e8b3474825eb83b5e0ec727e31e31cec3a40.png" />
</div>
</div>
<p>Training is completed after a few seconds!</p>
<p>Just for fun and because we can, let us save the trained point approximator to disk.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">checkpoint_path</span> <span class="o">=</span> <span class="s2">&quot;model.keras&quot;</span>
<span class="n">keras</span><span class="o">.</span><span class="n">saving</span><span class="o">.</span><span class="n">save_model</span><span class="p">(</span><span class="n">point_inference_workflow</span><span class="o">.</span><span class="n">approximator</span><span class="p">,</span> <span class="n">checkpoint_path</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Now we load the approximator again from disk and use it for inference and diagnosis below.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">loaded</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">saving</span><span class="o">.</span><span class="n">load_model</span><span class="p">(</span><span class="n">checkpoint_path</span><span class="p">)</span>
<span class="n">point_inference_workflow</span><span class="o">.</span><span class="n">approximator</span> <span class="o">=</span> <span class="n">loaded</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/ho/programs/anaconda3/envs/bayesflow/lib/python3.11/site-packages/keras/src/saving/serialization_lib.py:734: UserWarning: `compile()` was not called as part of model loading because the model&#39;s `compile()` method is custom. All subclassed Models that have `compile()` overridden should also override `get_compile_config()` and `compile_from_config(config)`. Alternatively, you can call `compile()` manually after loading.
  instance.compile_from_config(compile_config)
</pre></div>
</div>
</div>
</div>
<section id="inference">
<h4><span class="section-number">7.2.2.1. </span>Inference<a class="headerlink" href="#inference" title="Link to this heading">#</a></h4>
<p>The computational cost we have payed for training up front is amortized by cheap inference on simulated or measured observations.
This means, we can rapidly evaluate posteriors for different observations not seen in training, which allows for comprehensive diagnosis of posterior quality.</p>
<p>So far so general, but point estimators in particular give a speed advantage not only in training, but also with respect to diagnostics.
Since one point estimate already summarizes many posterior samples, we only have to do one forward pass with a point inference network, where we would have to make ~100 passes with a generative, full posterior approximator.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Simulate validation data</span>
<span class="n">val_sims</span> <span class="o">=</span> <span class="n">simulator</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="mi">500</span><span class="p">)</span>

<span class="c1"># estimate posteriors for all conditions </span>
<span class="n">estimates_point</span> <span class="o">=</span> <span class="n">point_inference_workflow</span><span class="o">.</span><span class="n">approximator</span><span class="o">.</span><span class="n">estimate</span><span class="p">(</span><span class="n">conditions</span><span class="o">=</span><span class="n">val_sims</span><span class="p">)</span>

<span class="c1"># `approximator.estimate()` returned a nested dictionary of point estimates for each named parameter,</span>
<span class="c1"># see the structure and shape below</span>
<span class="n">keras</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="n">keras</span><span class="o">.</span><span class="n">ops</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">estimates_point</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>{&#39;alpha&#39;: {&#39;mean&#39;: (500, 1), &#39;quantiles&#39;: (500, 5, 1)},
 &#39;beta&#39;: {&#39;mean&#39;: (500, 1), &#39;quantiles&#39;: (500, 5, 1)},
 &#39;gamma&#39;: {&#39;mean&#39;: (500, 1), &#39;quantiles&#39;: (500, 5, 1)},
 &#39;delta&#39;: {&#39;mean&#39;: (500, 1), &#39;quantiles&#39;: (500, 5, 1)}}
</pre></div>
</div>
</div>
</div>
</section>
<section id="recovery-and-calibration-diagnostics-for-point-estimates">
<h4><span class="section-number">7.2.2.2. </span>Recovery and calibration diagnostics for point estimates<a class="headerlink" href="#recovery-and-calibration-diagnostics-for-point-estimates" title="Link to this heading">#</a></h4>
<p>Diagnosing problems with point estimation is done similarly to full posterior approximation. For example, you can check how point estimates relate to ground truth values with a recovery plot. The recovery plot can be used for many different point estimates. Just define which point estimate is displayed with what kind of <a class="reference external" href="https://matplotlib.org/stable/api/markers_api.html">matplotlib marker</a> in a dictionary.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">marker_mapping</span> <span class="o">=</span> <span class="nb">dict</span><span class="p">(</span><span class="n">quantiles</span><span class="o">=</span><span class="s2">&quot;_&quot;</span><span class="p">,</span> <span class="n">mean</span><span class="o">=</span><span class="s2">&quot;*&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Above we defined horizontal bars to indicate quantile estimates and a star to indicate the estimated mean. Point estimates for the same condition are connected with a line.</p>
<p>We can provide pretty names to plotting functions so we define them once here:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">par_names</span> <span class="o">=</span> <span class="p">[</span><span class="sa">r</span><span class="s2">&quot;$\alpha$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\beta$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\gamma$&quot;</span><span class="p">,</span> <span class="sa">r</span><span class="s2">&quot;$\delta$&quot;</span><span class="p">]</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">recovery_from_estimates</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">estimates_point</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">marker_mapping</span><span class="o">=</span><span class="n">marker_mapping</span><span class="p">,</span>
    <span class="n">s</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span>  <span class="c1"># size of markers as in matplotlib.scatter</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/da697c20a59090dbacd825b2f4bed0ec416e5f09527becec34ac4a8afd438a15.png" src="../_images/da697c20a59090dbacd825b2f4bed0ec416e5f09527becec34ac4a8afd438a15.png" />
</div>
</div>
<p>We can and should also perform simulation based calibration checks on the estimated quantiles.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">calibration_ecdf_from_quantiles</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">estimates_point</span><span class="p">,</span> 
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">quantile_levels</span><span class="o">=</span><span class="n">q_levels</span><span class="p">,</span>
    <span class="n">difference</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/66efa5017b18455501b56055826e5fca636904a4e0c926c72d48708fd26a7ada.png" src="../_images/66efa5017b18455501b56055826e5fca636904a4e0c926c72d48708fd26a7ada.png" />
</div>
</div>
<p>Neither the recovery nor the calibration diagnostic indicates any problems with the point inferences. Let us go one step further in validation by checking the posterior predictive distribution.</p>
</section>
<section id="posterior-predictive-check-from-quantile-estimates">
<h4><span class="section-number">7.2.2.3. </span>Posterior predictive check from quantile estimates<a class="headerlink" href="#posterior-predictive-check-from-quantile-estimates" title="Link to this heading">#</a></h4>
<p>To sample the posterior we need to assume some concrete probability function. We will choose a diagonal multivariate normal distribution that we construct to be consistent with the quantile estimates.</p>
<p>More concretely, we calculate a mean and standard deviation for every parameter based on its outer most quantile estimates, that is quantile level 0.1 and 0.9.</p>
<p>We start by extracting the lower and upper bound from the quantile posterior approximation:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">post_bounds_from_quantiles</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">[:,[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]],</span> <span class="p">{</span><span class="n">k</span><span class="p">:</span><span class="n">v</span><span class="p">[</span><span class="s2">&quot;quantiles&quot;</span><span class="p">]</span> <span class="k">for</span> <span class="n">k</span><span class="p">,</span><span class="n">v</span> <span class="ow">in</span> <span class="n">estimates_point</span><span class="o">.</span><span class="n">items</span><span class="p">()})</span>
</pre></div>
</div>
</div>
</div>
<p>To translate these estimates to a corresponding mean and standard deviation we consider first the standard normal distribution. We know that we are interested in a translated and scaled version of it and since this is a linear transformation, we can calculate interpolation values <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> on the standard normal distribution and obtain mean and standard deviation for the normal distribution of interest.</p>
<p>If <span class="math notranslate nohighlight">\(X\)</span> follows a standard normal distribution with the known cumulative distribution function <span class="math notranslate nohighlight">\(F_X(x)\)</span>, the quantile for the quantile level <span class="math notranslate nohighlight">\(\tau_i\)</span> is <span class="math notranslate nohighlight">\(\tilde q_i = F_X^{-1}(\tau_{i})\)</span> and we can compute it for both quantile levels corresponding to the bounds we computed above.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># translate two quantile levels (first and last) to quantiles on the standard normal (mean=0, std=1)</span>
<span class="n">stdnormal_q</span> <span class="o">=</span> <span class="n">scipy</span><span class="o">.</span><span class="n">stats</span><span class="o">.</span><span class="n">norm</span><span class="o">.</span><span class="n">ppf</span><span class="p">(</span><span class="n">q_levels</span><span class="p">[[</span><span class="mi">0</span><span class="p">,</span><span class="o">-</span><span class="mi">1</span><span class="p">]])</span>
<span class="n">stdnormal_q</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([-1.28155157,  1.28155157])
</pre></div>
</div>
</div>
</div>
<p>In relation to <span class="math notranslate nohighlight">\(q_1\)</span> and <span class="math notranslate nohighlight">\(q_2\)</span>, where is <span class="math notranslate nohighlight">\(x=0\)</span> and <span class="math notranslate nohighlight">\(x=1\)</span>? These two correspond to location (mean) and scale (standard deviation) of the standard normal.</p>
<p>So we solve the equations</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    0 &amp;= \tilde q_1 (1-\alpha) + \tilde  q_2 \alpha,\\
    1 &amp;= \tilde q_1 (1-\beta) + \tilde  q_2 \beta,
\end{aligned}
\end{split}\]</div>
<p>for <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> and obtain</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \alpha &amp;= \frac {\tilde q_1} {\tilde q_1 - \tilde q_2},\\
    \beta &amp;= \frac {\tilde q_1 - 1} {\tilde q_1 - \tilde q_2}.
\end{aligned}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># calculate interpolation value for mean and standard deviation</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">stdnormal_q</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">/</span> <span class="p">(</span><span class="n">stdnormal_q</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">stdnormal_q</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>       <span class="c1"># interpolation value for q=0 (mean = 0 for standard normal)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="p">(</span><span class="n">stdnormal_q</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="mi">1</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">stdnormal_q</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">stdnormal_q</span><span class="p">[</span><span class="mi">1</span><span class="p">])</span>  <span class="c1"># interpolation value for q=1 (mean+std = 1 for standard normal)</span>
</pre></div>
</div>
</div>
</div>
<p>Since the standard normal and the normal distribution of interest are connected by a <em>linear</em> transformation, we can use the interpolation values <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> in the mean and standard deviation consistent with the selected quantile estimates as</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{aligned}
    \mu &amp;= \hat q_1 (1-\alpha) + \hat q_2 \alpha,\\
    \sigma &amp;= \hat q_1 (\alpha-\beta) + \hat q_2 (\beta-\alpha).
\end{aligned}
\end{split}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># interpolate between values to get scaled normal parameters</span>
<span class="n">post_means_from_quantiles</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">,</span> <span class="n">post_bounds_from_quantiles</span><span class="p">)</span>
<span class="n">post_stds_from_quantiles</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">beta</span><span class="o">-</span><span class="n">alpha</span><span class="p">),</span> <span class="n">post_bounds_from_quantiles</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>And finally we can sample from this normal distribution too.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">num_samples</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># sample from normal distribution consistent with quantile estimates</span>
<span class="n">post_draws_from_quantiles</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">rng</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span>
    <span class="n">loc</span><span class="o">=</span><span class="n">v</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">alpha</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="n">alpha</span><span class="p">,</span> 
    <span class="n">scale</span><span class="o">=</span><span class="n">v</span><span class="p">[:,</span><span class="mi">0</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">alpha</span><span class="o">-</span><span class="n">beta</span><span class="p">)</span> <span class="o">+</span> <span class="n">v</span><span class="p">[:,</span><span class="mi">1</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">beta</span><span class="o">-</span><span class="n">alpha</span><span class="p">),</span>
    <span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="mi">500</span><span class="p">,</span> <span class="n">num_samples</span><span class="p">))[</span><span class="o">...</span><span class="p">,</span> <span class="kc">None</span><span class="p">],</span> <span class="n">post_bounds_from_quantiles</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Let us take a look at a particular posterior. We could use any simulated or observed dataset now. For convenience, the BayesFlow diagnostic plots applicable to single dataset generally support passing a <code class="docutils literal notranslate"><span class="pre">dataset_id</span></code> to select one from the simulator output.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">dataset_id</span> <span class="o">=</span> <span class="mi">0</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">pairs_posterior</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">post_draws_from_quantiles</span><span class="p">,</span>
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset_id</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="k">def</span><span class="w"> </span><span class="nf">plot_boxes</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">boxes</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;blue&quot;</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">i</span><span class="p">,(</span><span class="n">key</span><span class="p">,</span> <span class="n">box</span><span class="p">)</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">boxes</span><span class="o">.</span><span class="n">items</span><span class="p">()):</span>
        <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">4</span><span class="p">):</span>
            <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">box</span><span class="p">[</span><span class="n">dataset_id</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
            <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="n">j</span><span class="p">,</span><span class="n">i</span><span class="p">]</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">box</span><span class="p">[</span><span class="n">dataset_id</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
            <span class="k">if</span> <span class="n">i</span> <span class="o">!=</span> <span class="n">j</span><span class="p">:</span>
                <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">box</span><span class="p">[</span><span class="n">dataset_id</span><span class="p">,</span><span class="mi">0</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>
                <span class="n">g</span><span class="o">.</span><span class="n">axes</span><span class="p">[</span><span class="n">i</span><span class="p">,</span><span class="n">j</span><span class="p">]</span><span class="o">.</span><span class="n">axhline</span><span class="p">(</span><span class="n">box</span><span class="p">[</span><span class="n">dataset_id</span><span class="p">,</span><span class="mi">1</span><span class="p">,</span><span class="mi">0</span><span class="p">],</span> <span class="n">color</span><span class="o">=</span><span class="n">color</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">)</span>

<span class="n">plot_boxes</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">post_bounds_from_quantiles</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">)</span>
<span class="n">g</span><span class="o">.</span><span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Posterior diagonal normal approximation&quot;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="mf">1.01</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f45d3bc538e934a0c512dc1ebed1ed4a4d3114576ceb4379c059ef940740c685.png" src="../_images/f45d3bc538e934a0c512dc1ebed1ed4a4d3114576ceb4379c059ef940740c685.png" />
</div>
</div>
<p>The dotted lines above are the estimated quantiles for the levels 0.1 and 0.9 and we see that the quantile based normal distribution generates consistent samples. Next, let us look at how the trajectories look like that correspond to parameters from this posterior.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">offline_posterior_sampler</span><span class="p">(</span><span class="n">post_draws</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">,</span> <span class="n">sample_id</span><span class="p">):</span>
    <span class="n">posterior_sample_for_id</span> <span class="o">=</span> <span class="p">{</span><span class="n">var_key</span><span class="p">:</span> <span class="n">post_draws</span><span class="p">[</span><span class="n">var_key</span><span class="p">][</span><span class="n">dataset_id</span><span class="p">,</span> <span class="n">sample_id</span><span class="p">,</span><span class="o">...</span><span class="p">]</span><span class="o">.</span><span class="n">squeeze</span><span class="p">()</span> <span class="k">for</span> <span class="n">var_key</span> <span class="ow">in</span> <span class="n">post_draws</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
    <span class="k">return</span> <span class="n">posterior_sample_for_id</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span><span class="w"> </span><span class="nf">take_dataset</span><span class="p">(</span><span class="n">sims</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">):</span>
    <span class="k">return</span> <span class="p">{</span><span class="n">var_key</span><span class="p">:</span> <span class="n">sims</span><span class="p">[</span><span class="n">var_key</span><span class="p">][</span><span class="n">dataset_id</span><span class="p">]</span> <span class="k">for</span> <span class="n">var_key</span> <span class="ow">in</span> <span class="n">sims</span><span class="o">.</span><span class="n">keys</span><span class="p">()}</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">list_of_resimulations</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sample_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
    <span class="n">one_post_sample</span> <span class="o">=</span> <span class="n">offline_posterior_sampler</span><span class="p">(</span><span class="n">post_draws_from_quantiles</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">,</span> <span class="n">sample_id</span><span class="p">)</span>
    <span class="n">list_of_resimulations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ecology_model</span><span class="p">(</span><span class="n">t_span</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span> <span class="o">**</span><span class="n">one_post_sample</span><span class="p">))</span>
<span class="n">resimulation_samples</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">tree_stack</span><span class="p">(</span><span class="n">list_of_resimulations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">observations</span> <span class="o">=</span> <span class="n">take_dataset</span><span class="p">(</span><span class="n">val_sims</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">)</span>

<span class="n">plot_trajectores</span><span class="p">(</span><span class="n">resimulation_samples</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Prey&quot;</span><span class="p">,</span> <span class="s2">&quot;Predator&quot;</span><span class="p">],</span> <span class="n">observations</span><span class="o">=</span><span class="n">observations</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Trajectories from posterior predictive distribution (diagonal normal approximation)&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1c70a51002f54c569528bf2c3edfd76c70b047a2f54b71f9796973349e79167d.png" src="../_images/1c70a51002f54c569528bf2c3edfd76c70b047a2f54b71f9796973349e79167d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_trajectores</span><span class="p">(</span><span class="n">resimulation_samples</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Prey&quot;</span><span class="p">,</span> <span class="s2">&quot;Predator&quot;</span><span class="p">],</span> <span class="n">observations</span><span class="o">=</span><span class="n">observations</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior predictive forecast (diagonal normal approximation)&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c5c4806abbcf9bf921d361c67f19ae214a779239b41defcc4dd5bf7ec900e8e2.png" src="../_images/c5c4806abbcf9bf921d361c67f19ae214a779239b41defcc4dd5bf7ec900e8e2.png" />
</div>
</div>
<p>The trajectories appear to fit well to the observations. Compare this to the prior predictive distribution from above. The predictive distribution now only contains trajectories with reasonable period, lag and scale. In this sense we already were successful in updating our knowledge about possible Lotka-Volterra parameters that fit to the data.</p>
<p>If any issues are visible in the posterior diagnostics, we could now go back, make a change to the simulator to better match real world observations, add relevant expert statistics, or try simple learnt statistics. Then, we train and diagnose again and repeat until the point estimates seem trustworthy.</p>
<p>Bear in mind however, that while the approximations allowed us to iterate fast they also come with a cost. By neglecting multimodality and correlation the approximate posterior is likely to be undercontracted (overdispersed). The next sections will step by step remove those approximations. Because we already know what to expect from the model, we can move confidently towards more complicated and powerful posterior approximation methods.</p>
</section>
</section>
</section>
<section id="full-posterior-approximation">
<h2><span class="section-number">7.3. </span>Full posterior approximation<a class="headerlink" href="#full-posterior-approximation" title="Link to this heading">#</a></h2>
<p>Flow Matching is a powerful class of generative neural networks. Let try and see if we can use it as a drop-in replacement for the <code class="docutils literal notranslate"><span class="pre">PointInferenceNetwork</span></code> we used previously.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">flow_matching</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">FlowMatching</span><span class="p">()</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">flow_matching_workflow</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">BasicWorkflow</span><span class="p">(</span>
    <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
    <span class="n">adapter</span><span class="o">=</span><span class="n">adapter</span><span class="p">,</span>
    <span class="n">inference_network</span><span class="o">=</span><span class="n">flow_matching</span><span class="p">,</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>Yes, we can!</p>
<p>We already know how to fit such a workflow. Flow matching performs well if you train it for a while. This takes a bit of time, but we will be rewarded by a tighter posterior approximation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">epochs</span> <span class="o">=</span> <span class="mi">50</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">flow_matching_workflow</span><span class="o">.</span><span class="n">fit_offline</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">,</span> 
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_data</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:bayesflow:Fitting on dataset instance of OfflineDataset.
INFO:bayesflow:Building on a test batch.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">4s</span> 4ms/step - loss: 1.5124 - loss/inference_loss: 1.5124 - val_loss: 0.6980 - val_loss/inference_loss: 0.6980
Epoch 2/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.7534 - loss/inference_loss: 0.7534 - val_loss: 0.5995 - val_loss/inference_loss: 0.5995
Epoch 3/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.6676 - loss/inference_loss: 0.6676 - val_loss: 0.5080 - val_loss/inference_loss: 0.5080
Epoch 4/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.6240 - loss/inference_loss: 0.6240 - val_loss: 0.5995 - val_loss/inference_loss: 0.5995
Epoch 5/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.5956 - loss/inference_loss: 0.5956 - val_loss: 0.5689 - val_loss/inference_loss: 0.5689
Epoch 6/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.5793 - loss/inference_loss: 0.5793 - val_loss: 0.5531 - val_loss/inference_loss: 0.5531
Epoch 7/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.5607 - loss/inference_loss: 0.5607 - val_loss: 0.4960 - val_loss/inference_loss: 0.4960
Epoch 8/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.5426 - loss/inference_loss: 0.5426 - val_loss: 0.4603 - val_loss/inference_loss: 0.4603
Epoch 9/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.5250 - loss/inference_loss: 0.5250 - val_loss: 0.3366 - val_loss/inference_loss: 0.3366
Epoch 10/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.5262 - loss/inference_loss: 0.5262 - val_loss: 0.5738 - val_loss/inference_loss: 0.5738
Epoch 11/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.5136 - loss/inference_loss: 0.5136 - val_loss: 0.4002 - val_loss/inference_loss: 0.4002
Epoch 12/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.5051 - loss/inference_loss: 0.5051 - val_loss: 0.4794 - val_loss/inference_loss: 0.4794
Epoch 13/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4940 - loss/inference_loss: 0.4940 - val_loss: 0.3145 - val_loss/inference_loss: 0.3145
Epoch 14/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4937 - loss/inference_loss: 0.4937 - val_loss: 0.4962 - val_loss/inference_loss: 0.4962
Epoch 15/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4850 - loss/inference_loss: 0.4850 - val_loss: 0.3971 - val_loss/inference_loss: 0.3971
Epoch 16/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4830 - loss/inference_loss: 0.4830 - val_loss: 0.5649 - val_loss/inference_loss: 0.5649
Epoch 17/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4813 - loss/inference_loss: 0.4813 - val_loss: 0.3687 - val_loss/inference_loss: 0.3687
Epoch 18/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4741 - loss/inference_loss: 0.4741 - val_loss: 0.4990 - val_loss/inference_loss: 0.4990
Epoch 19/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4682 - loss/inference_loss: 0.4682 - val_loss: 0.4729 - val_loss/inference_loss: 0.4729
Epoch 20/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4674 - loss/inference_loss: 0.4674 - val_loss: 0.3830 - val_loss/inference_loss: 0.3830
Epoch 21/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4646 - loss/inference_loss: 0.4646 - val_loss: 0.4645 - val_loss/inference_loss: 0.4645
Epoch 22/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4581 - loss/inference_loss: 0.4581 - val_loss: 0.3550 - val_loss/inference_loss: 0.3550
Epoch 23/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4565 - loss/inference_loss: 0.4565 - val_loss: 0.3952 - val_loss/inference_loss: 0.3952
Epoch 24/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4476 - loss/inference_loss: 0.4476 - val_loss: 0.4066 - val_loss/inference_loss: 0.4066
Epoch 25/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4506 - loss/inference_loss: 0.4506 - val_loss: 0.3373 - val_loss/inference_loss: 0.3373
Epoch 26/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4508 - loss/inference_loss: 0.4508 - val_loss: 0.4216 - val_loss/inference_loss: 0.4216
Epoch 27/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4459 - loss/inference_loss: 0.4459 - val_loss: 0.4444 - val_loss/inference_loss: 0.4444
Epoch 28/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4420 - loss/inference_loss: 0.4420 - val_loss: 0.4631 - val_loss/inference_loss: 0.4631
Epoch 29/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4389 - loss/inference_loss: 0.4389 - val_loss: 0.3162 - val_loss/inference_loss: 0.3162
Epoch 30/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4400 - loss/inference_loss: 0.4400 - val_loss: 0.4388 - val_loss/inference_loss: 0.4388
Epoch 31/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4393 - loss/inference_loss: 0.4393 - val_loss: 0.5072 - val_loss/inference_loss: 0.5072
Epoch 32/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4321 - loss/inference_loss: 0.4321 - val_loss: 0.4542 - val_loss/inference_loss: 0.4542
Epoch 33/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4361 - loss/inference_loss: 0.4361 - val_loss: 0.5522 - val_loss/inference_loss: 0.5522
Epoch 34/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4313 - loss/inference_loss: 0.4313 - val_loss: 0.5288 - val_loss/inference_loss: 0.5288
Epoch 35/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4285 - loss/inference_loss: 0.4285 - val_loss: 0.3008 - val_loss/inference_loss: 0.3008
Epoch 36/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4277 - loss/inference_loss: 0.4277 - val_loss: 0.3166 - val_loss/inference_loss: 0.3166
Epoch 37/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4253 - loss/inference_loss: 0.4253 - val_loss: 0.3985 - val_loss/inference_loss: 0.3985
Epoch 38/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4233 - loss/inference_loss: 0.4233 - val_loss: 0.3292 - val_loss/inference_loss: 0.3292
Epoch 39/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4196 - loss/inference_loss: 0.4196 - val_loss: 0.3764 - val_loss/inference_loss: 0.3764
Epoch 40/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4198 - loss/inference_loss: 0.4198 - val_loss: 0.3052 - val_loss/inference_loss: 0.3052
Epoch 41/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4230 - loss/inference_loss: 0.4230 - val_loss: 0.3368 - val_loss/inference_loss: 0.3368
Epoch 42/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4173 - loss/inference_loss: 0.4173 - val_loss: 0.3620 - val_loss/inference_loss: 0.3620
Epoch 43/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4180 - loss/inference_loss: 0.4180 - val_loss: 0.3729 - val_loss/inference_loss: 0.3729
Epoch 44/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4191 - loss/inference_loss: 0.4191 - val_loss: 0.3472 - val_loss/inference_loss: 0.3472
Epoch 45/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4177 - loss/inference_loss: 0.4177 - val_loss: 0.2900 - val_loss/inference_loss: 0.2900
Epoch 46/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4224 - loss/inference_loss: 0.4224 - val_loss: 0.3688 - val_loss/inference_loss: 0.3688
Epoch 47/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4206 - loss/inference_loss: 0.4206 - val_loss: 0.4123 - val_loss/inference_loss: 0.4123
Epoch 48/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4196 - loss/inference_loss: 0.4196 - val_loss: 0.4254 - val_loss/inference_loss: 0.4254
Epoch 49/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4142 - loss/inference_loss: 0.4142 - val_loss: 0.3533 - val_loss/inference_loss: 0.3533
Epoch 50/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">2s</span> 4ms/step - loss: 0.4234 - loss/inference_loss: 0.4234 - val_loss: 0.3729 - val_loss/inference_loss: 0.3729
CPU times: user 5min 11s, sys: 29.3 s, total: 5min 40s
Wall time: 1min 43s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e9f601af985cf23c7d561986b3717f2c7b447829d20bdaf05b8982ef214886b0.png" src="../_images/e9f601af985cf23c7d561986b3717f2c7b447829d20bdaf05b8982ef214886b0.png" />
</div>
</div>
<p>Sampling the flow matching approximator takes much longer than estimating with the point approximator. To save time, we restrict the number of inference conditions:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">val_sims</span> <span class="o">=</span> <span class="n">keras</span><span class="o">.</span><span class="n">tree</span><span class="o">.</span><span class="n">map_structure</span><span class="p">(</span><span class="k">lambda</span> <span class="n">v</span><span class="p">:</span> <span class="n">v</span><span class="p">[:</span><span class="mi">100</span><span class="p">],</span> <span class="n">val_sims</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="c1"># Set the number of posterior draws you want to get</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Obtain posterior draws with the sample method</span>
<span class="n">post_draws</span> <span class="o">=</span> <span class="n">flow_matching_workflow</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">conditions</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>

<span class="c1"># post_draws is a dictionary of draws with one element per named parameters</span>
<span class="n">post_draws</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 6min 33s, sys: 38.1 s, total: 7min 11s
Wall time: 55.8 s
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;alpha&#39;, &#39;beta&#39;, &#39;gamma&#39;, &#39;delta&#39;])
</pre></div>
</div>
</div>
</div>
<p>Quickly training a point inference network</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">recovery</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">post_draws</span><span class="p">,</span> 
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/21dedae29672f79f6bfa2333c7e0d66329de64da7394481abbed9f6e77ff79fc.png" src="../_images/21dedae29672f79f6bfa2333c7e0d66329de64da7394481abbed9f6e77ff79fc.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">calibration_ecdf</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">post_draws</span><span class="p">,</span> 
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span><span class="p">,</span>
    <span class="n">difference</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rank_type</span><span class="o">=</span><span class="s2">&quot;distance&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a89d297af2fb4e710d1afc028624d8bc5a17c6dab6c680ee40c715ef48234ed4.png" src="../_images/a89d297af2fb4e710d1afc028624d8bc5a17c6dab6c680ee40c715ef48234ed4.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">pairs_posterior</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">post_draws</span><span class="p">,</span> 
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset_id</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plot_boxes</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">post_bounds_from_quantiles</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/0ebec13a5f79650384600057fedcbd1d8a1529fffc2641fe9316bf70ca385524.png" src="../_images/0ebec13a5f79650384600057fedcbd1d8a1529fffc2641fe9316bf70ca385524.png" />
</div>
</div>
<p>Compared to the earlier approximate posterior draws we uncovered a strong correlation between parameters. Take a look at the marginals on the diagonal - the dotted quantile estimates still pass a visual consistency check.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">list_of_resimulations</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sample_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
    <span class="n">one_post_sample</span> <span class="o">=</span> <span class="n">offline_posterior_sampler</span><span class="p">(</span><span class="n">post_draws</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">,</span> <span class="n">sample_id</span><span class="p">)</span>
    <span class="n">list_of_resimulations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ecology_model</span><span class="p">(</span><span class="n">t_span</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span> <span class="o">**</span><span class="n">one_post_sample</span><span class="p">))</span>
<span class="n">resimulation_samples</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">tree_stack</span><span class="p">(</span><span class="n">list_of_resimulations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">observations</span> <span class="o">=</span> <span class="n">take_dataset</span><span class="p">(</span><span class="n">val_sims</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">)</span>

<span class="n">plot_trajectores</span><span class="p">(</span><span class="n">resimulation_samples</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Prey&quot;</span><span class="p">,</span> <span class="s2">&quot;Predator&quot;</span><span class="p">],</span> <span class="n">observations</span><span class="o">=</span><span class="n">observations</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Trajectories from posterior predictive distribution&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7c213c3634ec25f5ec02df93f535e13d81223f02135874ce51b3e83e0f91de46.png" src="../_images/7c213c3634ec25f5ec02df93f535e13d81223f02135874ce51b3e83e0f91de46.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_trajectores</span><span class="p">(</span><span class="n">resimulation_samples</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Prey&quot;</span><span class="p">,</span> <span class="s2">&quot;Predator&quot;</span><span class="p">],</span> <span class="n">observations</span><span class="o">=</span><span class="n">observations</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior predictive forecast&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/88a7c1a75724de01b02ec6eb2e6fa13c2637ffa230736a52b91210e9b2857f9d.png" src="../_images/88a7c1a75724de01b02ec6eb2e6fa13c2637ffa230736a52b91210e9b2857f9d.png" />
</div>
</div>
<p>Estimating the correlation of posterior samples has constrained the posterior predictive forecast uncertainty considerably!</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">z_score_contraction</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">post_draws</span><span class="p">,</span> 
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/59a980f235225f7a0a34646237f4675d537bfa9f8c46ae303e5bb0493ace4683.png" src="../_images/59a980f235225f7a0a34646237f4675d537bfa9f8c46ae303e5bb0493ace4683.png" />
</div>
</div>
</section>
<section id="end-to-end-learning-of-summary-statistics">
<h2><span class="section-number">7.4. </span>End-to-end learning of summary statistics<a class="headerlink" href="#end-to-end-learning-of-summary-statistics" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">summary_network</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">LSTNet</span><span class="p">()</span>  <span class="c1"># bf.networks.TimeSeriesTransformer() is slower, with similar performance on this task</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learnt_sumstat_adapter</span> <span class="o">=</span> <span class="p">(</span>
    <span class="n">bf</span><span class="o">.</span><span class="n">adapters</span><span class="o">.</span><span class="n">Adapter</span><span class="p">()</span>
    
    <span class="c1"># convert any non-arrays to numpy arrays</span>
    <span class="o">.</span><span class="n">to_array</span><span class="p">()</span>
    
    <span class="c1"># convert from numpy&#39;s default float64 to deep learning friendly float32</span>
    <span class="o">.</span><span class="n">convert_dtype</span><span class="p">(</span><span class="s2">&quot;float64&quot;</span><span class="p">,</span> <span class="s2">&quot;float32&quot;</span><span class="p">)</span>

    <span class="c1"># drop unobserved full trajectories</span>
    <span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">,</span> <span class="s2">&quot;t&quot;</span><span class="p">])</span>

    <span class="c1"># drop expert statistics</span>
    <span class="o">.</span><span class="n">drop</span><span class="p">([</span><span class="s2">&quot;means&quot;</span><span class="p">,</span> <span class="s2">&quot;log_vars&quot;</span><span class="p">,</span> <span class="s2">&quot;auto_corrs&quot;</span><span class="p">,</span> <span class="s2">&quot;cross_corr&quot;</span><span class="p">,</span> <span class="s2">&quot;period&quot;</span><span class="p">])</span>
    
    <span class="c1"># standardize target variables to zero mean and unit variance </span>
    <span class="o">.</span><span class="n">standardize</span><span class="p">(</span><span class="n">exclude</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;observed_x&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_y&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_t&quot;</span><span class="p">])</span>
    <span class="o">.</span><span class="n">as_time_series</span><span class="p">([</span><span class="s2">&quot;observed_x&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_y&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_t&quot;</span><span class="p">])</span>
    <span class="o">.</span><span class="n">standardize</span><span class="p">(</span><span class="n">include</span><span class="o">=</span><span class="p">[</span><span class="s2">&quot;observed_x&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_y&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_t&quot;</span><span class="p">],</span> <span class="n">axis</span><span class="o">=</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">1</span><span class="p">))</span> <span class="c1"># make sure to standardize whole timeseries</span>
    
    <span class="c1"># rename the variables to match the required approximator inputs</span>
    <span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="s2">&quot;alpha&quot;</span><span class="p">,</span> <span class="s2">&quot;beta&quot;</span><span class="p">,</span> <span class="s2">&quot;gamma&quot;</span><span class="p">,</span> <span class="s2">&quot;delta&quot;</span><span class="p">],</span> <span class="n">into</span><span class="o">=</span><span class="s2">&quot;inference_variables&quot;</span><span class="p">)</span>
    <span class="o">.</span><span class="n">concatenate</span><span class="p">([</span><span class="s2">&quot;observed_x&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_y&quot;</span><span class="p">,</span> <span class="s2">&quot;observed_t&quot;</span><span class="p">],</span> <span class="n">into</span><span class="o">=</span><span class="s2">&quot;summary_variables&quot;</span><span class="p">)</span>
    <span class="c1">#.concatenate([&quot;means&quot;, &quot;log_vars&quot;, &quot;auto_corrs&quot;, &quot;cross_corr&quot;, &quot;period&quot;], into=&quot;inference_conditions&quot;)</span>

<span class="p">)</span>
<span class="n">learnt_sumstat_adapter</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Adapter([0: ToArray -&gt; 1: ConvertDType -&gt; 2: Drop([&#39;x&#39;, &#39;y&#39;, &#39;t&#39;]) -&gt; 3: Drop([&#39;means&#39;, &#39;log_vars&#39;, &#39;auto_corrs&#39;, &#39;cross_corr&#39;, &#39;period&#39;]) -&gt; 4: Standardize(exclude=[&#39;observed_x&#39;, &#39;observed_y&#39;, &#39;observed_t&#39;]) -&gt; 5: AsTimeSeries -&gt; 6: Standardize(include=[&#39;observed_x&#39;, &#39;observed_y&#39;, &#39;observed_t&#39;]) -&gt; 7: Concatenate([&#39;alpha&#39;, &#39;beta&#39;, &#39;gamma&#39;, &#39;delta&#39;] -&gt; &#39;inference_variables&#39;) -&gt; 8: Concatenate([&#39;observed_x&#39;, &#39;observed_y&#39;, &#39;observed_t&#39;] -&gt; &#39;summary_variables&#39;)])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">learnt_sumstat_workflow</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">BasicWorkflow</span><span class="p">(</span>
    <span class="n">simulator</span><span class="o">=</span><span class="n">simulator</span><span class="p">,</span>
    <span class="n">adapter</span><span class="o">=</span><span class="n">learnt_sumstat_adapter</span><span class="p">,</span>
    <span class="n">summary_network</span><span class="o">=</span><span class="n">summary_network</span><span class="p">,</span>
    <span class="n">inference_network</span><span class="o">=</span><span class="n">bf</span><span class="o">.</span><span class="n">networks</span><span class="o">.</span><span class="n">FlowMatching</span><span class="p">(),</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="n">history</span> <span class="o">=</span> <span class="n">learnt_sumstat_workflow</span><span class="o">.</span><span class="n">fit_offline</span><span class="p">(</span>
    <span class="n">training_data</span><span class="p">,</span> 
    <span class="n">epochs</span><span class="o">=</span><span class="n">epochs</span><span class="p">,</span> 
    <span class="n">batch_size</span><span class="o">=</span><span class="n">batch_size</span><span class="p">,</span> 
    <span class="n">validation_data</span><span class="o">=</span><span class="n">validation_data</span>
<span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>INFO:bayesflow:Fitting on dataset instance of OfflineDataset.
INFO:bayesflow:Building on a test batch.
</pre></div>
</div>
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Epoch 1/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">13s</span> 13ms/step - loss: 1.1151 - loss/inference_loss: 1.1151 - val_loss: 0.6890 - val_loss/inference_loss: 0.6890
Epoch 2/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 11ms/step - loss: 0.6887 - loss/inference_loss: 0.6887 - val_loss: 0.5208 - val_loss/inference_loss: 0.5208
Epoch 3/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.5839 - loss/inference_loss: 0.5839 - val_loss: 0.4417 - val_loss/inference_loss: 0.4417
Epoch 4/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.5384 - loss/inference_loss: 0.5384 - val_loss: 0.5802 - val_loss/inference_loss: 0.5802
Epoch 5/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 11ms/step - loss: 0.5033 - loss/inference_loss: 0.5033 - val_loss: 0.3872 - val_loss/inference_loss: 0.3872
Epoch 6/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 11ms/step - loss: 0.4834 - loss/inference_loss: 0.4834 - val_loss: 0.4194 - val_loss/inference_loss: 0.4194
Epoch 7/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 11ms/step - loss: 0.4619 - loss/inference_loss: 0.4619 - val_loss: 0.3810 - val_loss/inference_loss: 0.3810
Epoch 8/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.4601 - loss/inference_loss: 0.4601 - val_loss: 0.4967 - val_loss/inference_loss: 0.4967
Epoch 9/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.4466 - loss/inference_loss: 0.4466 - val_loss: 0.5709 - val_loss/inference_loss: 0.5709
Epoch 10/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.4404 - loss/inference_loss: 0.4404 - val_loss: 0.5583 - val_loss/inference_loss: 0.5583
Epoch 11/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.4312 - loss/inference_loss: 0.4312 - val_loss: 0.3990 - val_loss/inference_loss: 0.3990
Epoch 12/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.4273 - loss/inference_loss: 0.4273 - val_loss: 0.2578 - val_loss/inference_loss: 0.2578
Epoch 13/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 11ms/step - loss: 0.4154 - loss/inference_loss: 0.4154 - val_loss: 0.4038 - val_loss/inference_loss: 0.4038
Epoch 14/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 11ms/step - loss: 0.4135 - loss/inference_loss: 0.4135 - val_loss: 0.4274 - val_loss/inference_loss: 0.4274
Epoch 15/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.4022 - loss/inference_loss: 0.4022 - val_loss: 0.3588 - val_loss/inference_loss: 0.3588
Epoch 16/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.4002 - loss/inference_loss: 0.4002 - val_loss: 0.5089 - val_loss/inference_loss: 0.5089
Epoch 17/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 12ms/step - loss: 0.4008 - loss/inference_loss: 0.4008 - val_loss: 0.3254 - val_loss/inference_loss: 0.3254
Epoch 18/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3949 - loss/inference_loss: 0.3949 - val_loss: 0.4541 - val_loss/inference_loss: 0.4541
Epoch 19/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3878 - loss/inference_loss: 0.3878 - val_loss: 0.3627 - val_loss/inference_loss: 0.3627
Epoch 20/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3889 - loss/inference_loss: 0.3889 - val_loss: 0.4500 - val_loss/inference_loss: 0.4500
Epoch 21/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3863 - loss/inference_loss: 0.3863 - val_loss: 0.5011 - val_loss/inference_loss: 0.5011
Epoch 22/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3817 - loss/inference_loss: 0.3817 - val_loss: 0.3433 - val_loss/inference_loss: 0.3433
Epoch 23/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3784 - loss/inference_loss: 0.3784 - val_loss: 0.2991 - val_loss/inference_loss: 0.2991
Epoch 24/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3672 - loss/inference_loss: 0.3672 - val_loss: 0.2965 - val_loss/inference_loss: 0.2965
Epoch 25/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3663 - loss/inference_loss: 0.3663 - val_loss: 0.3939 - val_loss/inference_loss: 0.3939
Epoch 26/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3598 - loss/inference_loss: 0.3598 - val_loss: 0.4299 - val_loss/inference_loss: 0.4299
Epoch 27/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3635 - loss/inference_loss: 0.3635 - val_loss: 0.3062 - val_loss/inference_loss: 0.3062
Epoch 28/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3632 - loss/inference_loss: 0.3632 - val_loss: 0.2916 - val_loss/inference_loss: 0.2916
Epoch 29/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3559 - loss/inference_loss: 0.3559 - val_loss: 0.2309 - val_loss/inference_loss: 0.2309
Epoch 30/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3499 - loss/inference_loss: 0.3499 - val_loss: 0.4021 - val_loss/inference_loss: 0.4021
Epoch 31/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3513 - loss/inference_loss: 0.3513 - val_loss: 0.3448 - val_loss/inference_loss: 0.3448
Epoch 32/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3504 - loss/inference_loss: 0.3504 - val_loss: 0.2802 - val_loss/inference_loss: 0.2802
Epoch 33/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3514 - loss/inference_loss: 0.3514 - val_loss: 0.4074 - val_loss/inference_loss: 0.4074
Epoch 34/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3450 - loss/inference_loss: 0.3450 - val_loss: 0.3862 - val_loss/inference_loss: 0.3862
Epoch 35/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 12ms/step - loss: 0.3423 - loss/inference_loss: 0.3423 - val_loss: 0.3064 - val_loss/inference_loss: 0.3064
Epoch 36/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3395 - loss/inference_loss: 0.3395 - val_loss: 0.3225 - val_loss/inference_loss: 0.3225
Epoch 37/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3423 - loss/inference_loss: 0.3423 - val_loss: 0.2851 - val_loss/inference_loss: 0.2851
Epoch 38/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 12ms/step - loss: 0.3413 - loss/inference_loss: 0.3413 - val_loss: 0.3146 - val_loss/inference_loss: 0.3146
Epoch 39/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3345 - loss/inference_loss: 0.3345 - val_loss: 0.4345 - val_loss/inference_loss: 0.4345
Epoch 40/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3387 - loss/inference_loss: 0.3387 - val_loss: 0.2540 - val_loss/inference_loss: 0.2540
Epoch 41/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3339 - loss/inference_loss: 0.3339 - val_loss: 0.3592 - val_loss/inference_loss: 0.3592
Epoch 42/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3328 - loss/inference_loss: 0.3328 - val_loss: 0.3567 - val_loss/inference_loss: 0.3567
Epoch 43/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3334 - loss/inference_loss: 0.3334 - val_loss: 0.2760 - val_loss/inference_loss: 0.2760
Epoch 44/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3279 - loss/inference_loss: 0.3279 - val_loss: 0.3351 - val_loss/inference_loss: 0.3351
Epoch 45/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3300 - loss/inference_loss: 0.3300 - val_loss: 0.3574 - val_loss/inference_loss: 0.3574
Epoch 46/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 11ms/step - loss: 0.3291 - loss/inference_loss: 0.3291 - val_loss: 0.3628 - val_loss/inference_loss: 0.3628
Epoch 47/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 11ms/step - loss: 0.3267 - loss/inference_loss: 0.3267 - val_loss: 0.4426 - val_loss/inference_loss: 0.4426
Epoch 48/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">5s</span> 11ms/step - loss: 0.3303 - loss/inference_loss: 0.3303 - val_loss: 0.1882 - val_loss/inference_loss: 0.1882
Epoch 49/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3295 - loss/inference_loss: 0.3295 - val_loss: 0.2819 - val_loss/inference_loss: 0.2819
Epoch 50/50
<span class=" -Color -Color-Bold">512/512</span> <span class=" -Color -Color-Green">━━━━━━━━━━━━━━━━━━━━</span> <span class=" -Color -Color-Bold">6s</span> 11ms/step - loss: 0.3313 - loss/inference_loss: 0.3313 - val_loss: 0.3821 - val_loss/inference_loss: 0.3821
CPU times: user 23min 35s, sys: 2min 10s, total: 25min 45s
Wall time: 4min 50s
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">f</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">loss</span><span class="p">(</span><span class="n">history</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1a44eb9bcc5444ec142aa5e0e126edcafaf3830fabffbf429b68fdd9b00afbde.png" src="../_images/1a44eb9bcc5444ec142aa5e0e126edcafaf3830fabffbf429b68fdd9b00afbde.png" />
</div>
</div>
<p>Note, that the loss is lower since we are learning summary statistics simultaneously. How does this translate to visual diagnostics? We can check them again by sampling the posteriors of validation simulations not seen in training.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="o">%%time</span>
<span class="c1"># Set the number of posterior draws you want to get</span>
<span class="n">num_samples</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Obtain posterior draws with the sample method</span>
<span class="n">post_draws</span> <span class="o">=</span> <span class="n">learnt_sumstat_workflow</span><span class="o">.</span><span class="n">sample</span><span class="p">(</span><span class="n">conditions</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span> <span class="n">num_samples</span><span class="o">=</span><span class="n">num_samples</span><span class="p">)</span>

<span class="c1"># post_draws is a dictionary of draws with one element per named parameters</span>
<span class="n">post_draws</span><span class="o">.</span><span class="n">keys</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>CPU times: user 5min 41s, sys: 47.6 s, total: 6min 28s
Wall time: 53.4 s
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>dict_keys([&#39;alpha&#39;, &#39;beta&#39;, &#39;gamma&#39;, &#39;delta&#39;])
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">recovery</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">post_draws</span><span class="p">,</span> 
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f7cf6615874599905363cf4399d8a71af2527c636d4cf5b4512bd595f3eb91b6.png" src="../_images/f7cf6615874599905363cf4399d8a71af2527c636d4cf5b4512bd595f3eb91b6.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">calibration_ecdf</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">post_draws</span><span class="p">,</span> 
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span><span class="p">,</span>
    <span class="n">difference</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span>
    <span class="n">rank_type</span><span class="o">=</span><span class="s2">&quot;distance&quot;</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/8a1e4e42e6ebd7642db8df349edcd33e2983341db2790f431fc51f888fd5e387.png" src="../_images/8a1e4e42e6ebd7642db8df349edcd33e2983341db2790f431fc51f888fd5e387.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">g</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">pairs_posterior</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">post_draws</span><span class="p">,</span> 
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">dataset_id</span><span class="o">=</span><span class="n">dataset_id</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span><span class="p">,</span>
<span class="p">)</span>
<span class="n">plot_boxes</span><span class="p">(</span><span class="n">g</span><span class="p">,</span> <span class="n">post_bounds_from_quantiles</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/43cfee205689bb0e6c183af975843f7252ace8b654ba44dca04e4ad2b0833f9f.png" src="../_images/43cfee205689bb0e6c183af975843f7252ace8b654ba44dca04e4ad2b0833f9f.png" />
</div>
</div>
<p>Since the conditions changed now that we learn summaries of observations simultaneously to fitting the inference network, it is not surprising that posteriors seem to be shifted. You can compare how the new posterior samples relate to the dotted quantile estimates.</p>
<p>Neither expert-crafted nor jointly learnt statistics are guaranteed to be highly informative. However, to get to the global minimum of the training loss, the statistics need to be maximally informative. If architecture, training data and optimizer are well chosen, learnt summary statistics regularly outperform hand-crafted statistics.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">list_of_resimulations</span> <span class="o">=</span> <span class="p">[]</span>
<span class="k">for</span> <span class="n">sample_id</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">num_samples</span><span class="p">):</span>
    <span class="n">one_post_sample</span> <span class="o">=</span> <span class="n">offline_posterior_sampler</span><span class="p">(</span><span class="n">post_draws</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">,</span> <span class="n">sample_id</span><span class="p">)</span>
    <span class="n">list_of_resimulations</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">ecology_model</span><span class="p">(</span><span class="n">t_span</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span><span class="mi">20</span><span class="p">],</span> <span class="o">**</span><span class="n">one_post_sample</span><span class="p">))</span>
<span class="n">resimulation_samples</span> <span class="o">=</span> <span class="n">bf</span><span class="o">.</span><span class="n">utils</span><span class="o">.</span><span class="n">tree_stack</span><span class="p">(</span><span class="n">list_of_resimulations</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="n">observations</span> <span class="o">=</span> <span class="n">take_dataset</span><span class="p">(</span><span class="n">val_sims</span><span class="p">,</span> <span class="n">dataset_id</span><span class="p">)</span>

<span class="n">plot_trajectores</span><span class="p">(</span><span class="n">resimulation_samples</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Prey&quot;</span><span class="p">,</span> <span class="s2">&quot;Predator&quot;</span><span class="p">],</span> <span class="n">observations</span><span class="o">=</span><span class="n">observations</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlim</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Trajectories from posterior predictive distribution&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/147c5837c1826b9fc89d7f5aeffcf5d6e177ba019475ac3f6d978144f087299d.png" src="../_images/147c5837c1826b9fc89d7f5aeffcf5d6e177ba019475ac3f6d978144f087299d.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">plot_trajectores</span><span class="p">(</span><span class="n">resimulation_samples</span><span class="p">,</span> <span class="p">[</span><span class="s2">&quot;x&quot;</span><span class="p">,</span> <span class="s2">&quot;y&quot;</span><span class="p">],</span> <span class="p">[</span><span class="s2">&quot;Prey&quot;</span><span class="p">,</span> <span class="s2">&quot;Predator&quot;</span><span class="p">],</span> <span class="n">observations</span><span class="o">=</span><span class="n">observations</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s2">&quot;Posterior predictive forecast&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="mi">5</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s2">&quot;grey&quot;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s2">&quot;:&quot;</span><span class="p">);</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/1235fecef38cdba27c40b7da15570d27e828147abe67fe09a7f7bf4a2914d7f3.png" src="../_images/1235fecef38cdba27c40b7da15570d27e828147abe67fe09a7f7bf4a2914d7f3.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">bf</span><span class="o">.</span><span class="n">diagnostics</span><span class="o">.</span><span class="n">plots</span><span class="o">.</span><span class="n">z_score_contraction</span><span class="p">(</span>
    <span class="n">estimates</span><span class="o">=</span><span class="n">post_draws</span><span class="p">,</span> 
    <span class="n">targets</span><span class="o">=</span><span class="n">val_sims</span><span class="p">,</span>
    <span class="n">variable_names</span><span class="o">=</span><span class="n">par_names</span>
<span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c236c071350bcb8927e716edf4cc084bd2b4a2b5289af921c1f313a28d0bc10f.png" src="../_images/c236c071350bcb8927e716edf4cc084bd2b4a2b5289af921c1f313a28d0bc10f.png" />
</div>
</div>
</section>
</section>


                </article>
              
              
              
              
              
                <footer class="prev-next-footer d-print-none">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="One_Sample_TTest.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">6. </span>Simple Model Comparison - One Sample T-Test</p>
      </div>
    </a>
    <a class="right-next"
       href="../api/bayesflow.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">API Reference</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <dialog id="pst-secondary-sidebar-modal"></dialog>
                <div id="pst-secondary-sidebar" class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
<div
    id="pst-page-navigation-heading-2"
    class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> On this page
  </div>
  <nav class="bd-toc-nav page-toc" aria-labelledby="pst-page-navigation-heading-2">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ecology-simulator">7.1. Ecology simulator</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#rapid-inference">7.2. Rapid inference</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#basic-hand-crafted-summary-statistics">7.2.1. Basic hand crafted summary statistics</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#point-estimation">7.2.2. Point estimation</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#inference">7.2.2.1. Inference</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#recovery-and-calibration-diagnostics-for-point-estimates">7.2.2.2. Recovery and calibration diagnostics for point estimates</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#posterior-predictive-check-from-quantile-estimates">7.2.2.3. Posterior predictive check from quantile estimates</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#full-posterior-approximation">7.3. Full posterior approximation</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#end-to-end-learning-of-summary-statistics">7.4. End-to-end learning of summary statistics</a></li>
</ul>
  </nav></div>

  <div class="sidebar-secondary-item">

  
  <div class="tocsection editthispage">
    <a href="https://github.com/bayesflow-org/bayesflow/edit/dev/docsrc/_examples/Lotka_Volterra_point_estimation_and_expert_stats.ipynb">
      <i class="fa-solid fa-pencil"></i>
      
      
        
          Edit on GitHub
        
      
    </a>
  </div>
</div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
          </footer>
        
      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script defer src="../_static/scripts/bootstrap.js?digest=8878045cc6db502f8baf"></script>
<script defer src="../_static/scripts/pydata-sphinx-theme.js?digest=8878045cc6db502f8baf"></script>

  <footer class="bd-footer">
<div class="bd-footer__inner bd-page-width">
  
    <div class="footer-items__start">
      
        <div class="footer-item">

  <p class="copyright">
    
      © Copyright 2023-2025, BayesFlow authors (lead maintainer: Stefan T. Radev).
      <br/>
    
  </p>
</div>
      
        <div class="footer-item">

  <p class="sphinx-version">
    Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.2.3.
    <br/>
  </p>
</div>
      
    </div>
  
  
  
    <div class="footer-items__end">
      
        <div class="footer-item">
<p class="theme-version">
  <!-- # L10n: Setting the PST URL as an argument as this does not need to be localized -->
  Built with the <a href="https://pydata-sphinx-theme.readthedocs.io/en/stable/index.html">PyData Sphinx Theme</a> 0.16.1.
</p></div>
      
    </div>
  
</div>

  </footer>
  </body>
</html>